@inproceedings{gaillochet_AutomatingMedSAMLearning_2024,
  title = {Automating {{MedSAM}} by {{Learning Prompts}} with {{Weak Few-Shot Supervision}}},
  booktitle = {Foundation {{Models}} for {{Medical Artificial General Intelligence}}},
  author = {Gaillochet, M{\'e}lanie and Desrosiers, Christian and Lombaert, Herv{\'e}},
  editor = {Deng, Zhongying and Shen, Yiqing and Kim, Hyunwoo J. and Jeong, Won-Ki and {Aviles-Rivero}, Angelica I. and He, Junjun and Zhang, Shaoting},
  year = {2024},
  pages = {61--70},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-73471-7_7},
  abstract = {Foundation models such as the recently introduced Segment Anything Model (SAM) have achieved remarkable results in image segmentation tasks. However, these models typically require user interaction through handcrafted prompts such as bounding boxes, which limits their deployment to downstream tasks. Adapting these models to a specific task with fully labeled data also demands expensive prior user interaction to obtain ground-truth annotations. This work proposes to replace conditioning on input prompts with a lightweight module that directly learns a prompt embedding from the image embedding, both of which are subsequently used by the foundation model to output a segmentation mask. Our foundation models with learnable prompts can automatically segment any specific region by 1) modifying the input through a prompt embedding predicted by a simple module, and 2) using weak labels (tight bounding boxes) and few-shot supervision (10 samples). Our approach is validated on MedSAM, a version of SAM fine-tuned for medical images, with results on three medical datasets in MR and ultrasound imaging. Our code is available on https://github.com/Minimel/MedSAMWeakFewShotPromptAutomation.},
  copyright = {All rights reserved},
  isbn = {978-3-031-73471-7},
  langid = {english},
  selected = {true},
  code = {https://github.com/Minimel/MedSAMWeakFewShotPromptAutomation.git}
}

@inbook{gaillochet_JointReconstructionBias_2020,
  title = {Joint {{Reconstruction}} and {{Bias Field Correction}} for {{Undersampled MR Imaging}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  year = {2020},
  pages = {44--52},
  publisher = {Springer International Publishing},
  address = {Cham},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-030-59713-9_5},
  urldate = {2025-07-10},
  author = {Gaillochet, M{\'e}lanie and Tezcan, Kerem C. and Konukoglu, Ender},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-030-59712-2 978-3-030-59713-9},
  langid = {english},
  abstract = {Undersampling the k-space in MRI allows saving precious acquisition time, yet results in an ill-posed inversion problem. Recently, many deep learning techniques have been developed, addressing this issue of recovering the fully sampled MR image from the undersampled data. However, these learning based schemes are susceptible to differences between the training data and the image to be reconstructed at test time. One such difference can be attributed to the bias field present in MR images, caused by field inhomogeneities and coil sensitivities. In this work, we address the sensitivity of the reconstruction problem to the bias field and propose to model it explicitly in the reconstruction, in order to decrease this sensitivity. To this end, we use an unsupervised learning based reconstruction algorithm as our basis and combine it with a N4-based bias field estimation method, in a joint optimization scheme. We use the HCP dataset as well as in-house measured images for the evaluations. We show that the proposed method improves the reconstruction quality, both visually and in terms of RMSE.},
  file = {/Users/nini/Zotero/storage/BEDYUWLH/2020_Joint Reconstruction and Bias Field Correction for Undersampled MR Imaging.pdf}
}

@article{gaillochet_PromptLearningBounding_2025,
  title = {Prompt Learning with Bounding Box Constraints for Medical Image Segmentation},
  author = {Gaillochet, M{\'e}lanie and Noori, Mehrdad and Dastani, Sahar and Desrosiers, Christian and Lombaert, Herv{\'e}},
  year = {2025},
  journal = {IEEE Transactions on Biomedical Engineering},
  pages = {1--10},
  issn = {1558-2531},
  doi = {10.1109/TBME.2025.3582749},
  urldate = {2025-07-10},
  abstract = {Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multi-modal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90\% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The selected will be available upon acceptance},
  copyright = {All rights reserved},
  file = {/Users/nini/Zotero/storage/9GP8ZJSW/Gaillochet et al_2025_Prompt learning with bounding box constraints for medical image segmentation.pdf},
  selected = {true},
  code = {https://github.com/Minimel/box-prompt-learning-VFM.git}
}

@inproceedings{gaillochet_TAALTestTimeAugmentation_2022,
  title = {{{TAAL}}: {{Test-Time Augmentation}} for~{{Active Learning}} in~{{Medical Image Segmentation}}},
  shorttitle = {{{TAAL}}},
  booktitle = {Data {{Augmentation}}, {{Labelling}}, and {{Imperfections}}},
  author = {Gaillochet, M{\'e}lanie and Desrosiers, Christian and Lombaert, Herv{\'e}},
  editor = {Nguyen, Hien V. and Huang, Sharon X. and Xue, Yuan},
  year = {2022},
  pages = {43--53},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-17027-0_5},
  abstract = {Deep learning methods typically depend on the availability of labeled data, which is expensive and time-consuming to obtain. Active learning addresses such effort by prioritizing which samples are best to annotate in order to maximize the performance of the task model. While frameworks for active learning have been widely explored in the context of classification of natural images, they have been only sparsely used in medical image segmentation. The challenge resides in obtaining an uncertainty measure that reveals the best candidate data for annotation. This paper proposes Test-time Augmentation for Active Learning (TAAL), a novel semi-supervised active learning approach for segmentation that exploits the uncertainty information offered by data transformations. Our method applies cross-augmentation consistency during training and inference to both improve model learning in a semi-supervised fashion and identify the most relevant unlabeled samples to annotate next. In addition, our consistency loss uses a modified version of the JSD to further improve model performance. By relying on data transformations rather than on external modules or simple heuristics typically used in uncertainty-based strategies, TAAL emerges as a simple, yet powerful task-agnostic semi-supervised active learning approach applicable to the medical domain. Our results on a publicly-available dataset of cardiac images show that TAAL outperforms existing baseline methods in both fully-supervised and semi-supervised settings. Our implementation is publicly available on https://github.com/melinphd/TAAL.},
  copyright = {All rights reserved},
  isbn = {978-3-031-17027-0},
  langid = {english},
  file = {/Users/nini/Zotero/storage/NUAGLXWY/Gaillochet et al_2022_TAAL.pdf},
  code = {https://github.com/Minimel/TAAL.git},
}

@article{GAILLOCHET2023102958,
  title = {Active Learning for Medical Image Segmentation with Stochastic Batches},
  author = {Gaillochet, M{\'e}lanie and Desrosiers, Christian and Lombaert, Herv{\'e}},
  year = {2023},
  journal = {Medical Image Analysis},
  volume = {90},
  pages = {102958},
  issn = {1361-8415},
  doi = {10.1016/j.media.2023.102958},
  abstract = {The performance of learning-based algorithms improves with the amount of labelled data used for training. Yet, manually annotating data is particularly difficult for medical image segmentation tasks because of the limited expert availability and intensive manual effort required. To reduce manual labelling, active learning (AL) targets the most informative samples from the unlabelled set to annotate and add to the labelled training set. On the one hand, most active learning works have focused on the classification or limited segmentation of natural images, despite active learning being highly desirable in the difficult task of medical image segmentation. On the other hand, uncertainty-based AL approaches notoriously offer sub-optimal batch-query strategies, while diversity-based methods tend to be computationally expensive. Over and above methodological hurdles, random sampling has proven an extremely difficult baseline to outperform when varying learning and sampling conditions. This work aims to take advantage of the diversity and speed offered by random sampling to improve the selection of uncertainty-based AL methods for segmenting medical images. More specifically, we propose to compute uncertainty at the level of batches instead of samples through an original use of stochastic batches (SB) during sampling in AL. Stochastic batch querying is a simple and effective add-on that can be used on top of any uncertainty-based metric. Extensive experiments on two medical image segmentation datasets show that our strategy consistently improves conventional uncertainty-based sampling methods. Our method can hence act as a strong baseline for medical image segmentation. The selected is available on: https://github.com/Minimel/StochasticBatchAL.git.},
  copyright = {All rights reserved},
  keywords = {ActiveLearning,Medical image analysis,Segmentation,Uncertainty},
  selected = {true},
  code = {https://github.com/Minimel/StochasticBatchAL.git},
}
